{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXf3lMShai459K1xiXIYkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GokuHashira/ActiveLoopBasics/blob/master/Activeloop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Activeloop and LLMs\n"
      ],
      "metadata": {
        "id": "g6t3nd2NUUDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Zero to Hero"
      ],
      "metadata": {
        "id": "-isf86AoUkqO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ytyDO5ZNjy3",
        "outputId": "70157f10-72fb-4b84-9044-feb45fa8830b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.208\n",
            "  Downloading langchain-0.0.208-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.208)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.13 (from langchain==0.0.208)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.208)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.208) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.208) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.208) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.208) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.208) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.208)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.208 langchainplus-sdk-0.0.20 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Collecting deeplake==3.6.5\n",
            "  Downloading deeplake-3.6.5.tar.gz (516 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.8/516.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deeplake==3.6.5) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from deeplake==3.6.5) (8.4.0)\n",
            "Collecting boto3 (from deeplake==3.6.5)\n",
            "  Downloading boto3-1.28.3-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake==3.6.5) (8.1.4)\n",
            "Collecting pathos (from deeplake==3.6.5)\n",
            "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake==3.6.5)\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake==3.6.5) (4.65.0)\n",
            "Collecting numcodecs (from deeplake==3.6.5)\n",
            "  Downloading numcodecs-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt (from deeplake==3.6.5)\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting aioboto3>=10.4.0 (from deeplake==3.6.5)\n",
            "  Downloading aioboto3-11.2.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake==3.6.5) (1.5.6)\n",
            "Collecting aiobotocore[boto3]==2.5.0 (from aioboto3>=10.4.0->deeplake==3.6.5)\n",
            "  Downloading aiobotocore-2.5.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.29.77,>=1.29.76 (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5)\n",
            "  Downloading botocore-1.29.76-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (3.8.4)\n",
            "Requirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (1.14.1)\n",
            "Collecting aioitertools>=0.5.1 (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake==3.6.5)\n",
            "  Downloading boto3-1.26.76-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake==3.6.5)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->deeplake==3.6.5)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from humbug>=0.3.1->deeplake==3.6.5) (2.27.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from numcodecs->deeplake==3.6.5) (0.4)\n",
            "Collecting ppft>=1.7.6.6 (from pathos->deeplake==3.6.5)\n",
            "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.6 (from pathos->deeplake==3.6.5)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.2 (from pathos->deeplake==3.6.5)\n",
            "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.14 (from pathos->deeplake==3.6.5)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake==3.6.5) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake==3.6.5) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake==3.6.5) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.77,>=1.29.76->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake==3.6.5) (1.16.0)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.6.5-py3-none-any.whl size=629780 sha256=6e7a9bb55cd76a483b71f47615a565d2cd5f8584227854f3472eb2b89636150b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/e0/30/650c80d746460e559d85d5aea950aafc0186abd80230ccf5fc\n",
            "Successfully built deeplake\n",
            "Installing collected packages: pyjwt, ppft, pox, numcodecs, jmespath, dill, aioitertools, multiprocess, humbug, botocore, s3transfer, pathos, aiobotocore, boto3, aioboto3, deeplake\n",
            "Successfully installed aioboto3-11.2.0 aiobotocore-2.5.0 aioitertools-0.11.0 boto3-1.26.76 botocore-1.29.76 deeplake-3.6.5 dill-0.3.6 humbug-0.3.2 jmespath-1.0.1 multiprocess-0.70.14 numcodecs-0.11.0 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 pyjwt-2.7.0 s3transfer-0.6.1\n",
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting tiktoken==0.4.0\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.208\n",
        "!pip install deeplake==3.6.5\n",
        "!pip install openai==0.27.8\n",
        "!pip install python-dotenv\n",
        "!pip install tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3k-1wXGTymr",
        "outputId": "471dba97-effe-4adc-f99a-aedf943999f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/gdrive/MyDrive/ActiveLoop/.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-kAwxPyQBDx",
        "outputId": "7a993169-4b90-49a5-a56f-374f7a3002ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjGpQa3gT5PL",
        "outputId": "b5e7f481-1de8-4e79-fe98-bffd160995d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.7) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li-P9QSQW7qG",
        "outputId": "2959e996-0dca-49dd-fce2-5694c0c4de03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Jog for 20 minutes at a steady pace twice a week.\n",
            "2. Take a brisk walk for 30 minutes three times a week.\n",
            "3. Swim laps at a moderate pace for 30 minutes three times a week.\n",
            "4. Participate in a football, basketball, or soccer game for one hour twice a week.\n",
            "5. Cycle for 30 minutes at moderate intensity on alternate days.\n",
            "6. Take part in a hiit or circuit training for 30 minutes twice a week.\n",
            "7. Use the stairs instead of the elevator when available.\n",
            "8. Climb a mountain or hike a challenging terrain at least once a month.\n",
            "9. Perform a challenging bodyweight workout for 30 minutes once a week.\n",
            "10. Try yoga or pilates once a week for increased flexibility and balance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest some advice to improve stamina\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYahb2LwXZXw",
        "outputId": "5fa3aa67-f840-487f-9626-fdd715d056b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Make sure to get enough sleep. Aim for 7-9 hours of quality sleep each night.\n",
            "\n",
            "2. Incorporate aerobic exercise into your daily routine. Aim for 30 minutes of aerobic exercise 3-4 times a week.\n",
            "\n",
            "3. Eat a balanced and nutritious diet with plenty of protein, vitamins, minerals, and antioxidants.\n",
            "\n",
            "4. Practice deep breathing and mindfulness techniques to reduce stress and improve your mental wellbeing.\n",
            "\n",
            "5. Drink plenty of fluids and practice proper hydration throughout the day.\n",
            "\n",
            "6. Incorporate strength training exercises into your workout routine to improve overall muscular endurance.\n",
            "\n",
            "7. Make sure to take rest days in order to give your body a chance to recover.\n",
            "\n",
            "8. Avoid unhealthy habits such as smoking, excessive alcohol consumption, and poor diet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature = 0.9)\n",
        "prompt = PromptTemplate(input_variables=[\"product\"], template = \"What is a good name for a company that makes {product}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(chain.run(\"eco-friendly water bottles\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgTK-wAYXjto",
        "outputId": "292cd17d-2061-41c1-c259-534cf95c2781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EcoHydro Bottles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature = 0.9)\n",
        "prompt = PromptTemplate(input_variables=[\"word\"], template = \"Translate this {word} to Tamil.\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(chain.run(\"Politics\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_qwXPFLZHt8",
        "outputId": "f3776ad3-6b3f-4691-b8d8-6b454599d57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "அரசியல்\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = OpenAI(model = \"text-davinci-003\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    verbose = True,\n",
        "    memory = ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"Tell me about yourself\")\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with Data Analysis?\")\n",
        "\n",
        "print(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoJUlUGMZpx4",
        "outputId": "99dae21c-1319-4076-9076-a7cc498fc95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself\n",
            "AI:  Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself\n",
            "AI:  Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\n",
            "Human: What can you do?\n",
            "AI:  I can help you with a variety of tasks. I can provide you with information from my context, such as the current weather, news, and more. I can also help you with scheduling tasks, setting reminders, and more. I'm constantly learning new things, so I'm always able to help you with more tasks. Is there anything specific you need help with?\n",
            "Human: How can you help me with Data Analysis?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\", additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=\" I can help you with a variety of tasks. I can provide you with information from my context, such as the current weather, news, and more. I can also help you with scheduling tasks, setting reminders, and more. I'm constantly learning new things, so I'm always able to help you with more tasks. Is there anything specific you need help with?\", additional_kwargs={}, example=False), HumanMessage(content='How can you help me with Data Analysis?', additional_kwargs={}, example=False), AIMessage(content=' I can help you with data analysis by providing you with insights and analysis of your data. I can also help you with data visualization, so you can better understand your data. Additionally, I can help you with data mining, so you can uncover hidden patterns and trends in your data.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Can send me a motivational quote from any famous Author you know?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "YGVOWfuYeY_R",
        "outputId": "3db0b651-64f4-4e45-e25d-22d9e0161406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself\n",
            "AI:  Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\n",
            "Human: What can you do?\n",
            "AI:  I can help you with a variety of tasks. I can provide you with information from my context, such as the current weather, news, and more. I can also help you with scheduling tasks, setting reminders, and more. I'm constantly learning new things, so I'm always able to help you with more tasks. Is there anything specific you need help with?\n",
            "Human: How can you help me with Data Analysis?\n",
            "AI:  I can help you with data analysis by providing you with insights and analysis of your data. I can also help you with data visualization, so you can better understand your data. Additionally, I can help you with data mining, so you can uncover hidden patterns and trends in your data.\n",
            "Human: How can send me a motivational quote from any famous Author you know?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sure! I can send you a motivational quote from any famous author I know. I have a database of quotes from famous authors that I can draw from. Would you like me to send you a quote now?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Yes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "XrSOsDRqezKH",
        "outputId": "1ffa2d76-55b8-42fb-cb9f-b09d120cf2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself\n",
            "AI:  Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\n",
            "Human: What can you do?\n",
            "AI:  I can help you with a variety of tasks. I can provide you with information from my context, such as the current weather, news, and more. I can also help you with scheduling tasks, setting reminders, and more. I'm constantly learning new things, so I'm always able to help you with more tasks. Is there anything specific you need help with?\n",
            "Human: How can you help me with Data Analysis?\n",
            "AI:  I can help you with data analysis by providing you with insights and analysis of your data. I can also help you with data visualization, so you can better understand your data. Additionally, I can help you with data mining, so you can uncover hidden patterns and trends in your data.\n",
            "Human: How can send me a motivational quote from any famous Author you know?\n",
            "AI:  Sure! I can send you a motivational quote from any famous author I know. I have a database of quotes from famous authors that I can draw from. Would you like me to send you a quote now?\n",
            "Human: Yes\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Absolutely! Here is a quote from Albert Einstein: \"The only source of knowledge is experience.\" I hope this quote inspires you!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"From which book did you get this quote from?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "ENYM5g7Pe3VQ",
        "outputId": "7e5dc165-e489-46c9-d709-315e72b53a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself\n",
            "AI:  Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\n",
            "Human: What can you do?\n",
            "AI:  I can help you with a variety of tasks. I can provide you with information from my context, such as the current weather, news, and more. I can also help you with scheduling tasks, setting reminders, and more. I'm constantly learning new things, so I'm always able to help you with more tasks. Is there anything specific you need help with?\n",
            "Human: How can you help me with Data Analysis?\n",
            "AI:  I can help you with data analysis by providing you with insights and analysis of your data. I can also help you with data visualization, so you can better understand your data. Additionally, I can help you with data mining, so you can uncover hidden patterns and trends in your data.\n",
            "Human: How can send me a motivational quote from any famous Author you know?\n",
            "AI:  Sure! I can send you a motivational quote from any famous author I know. I have a database of quotes from famous authors that I can draw from. Would you like me to send you a quote now?\n",
            "Human: Yes\n",
            "AI:   Absolutely! Here is a quote from Albert Einstein: \"The only source of knowledge is experience.\" I hope this quote inspires you!\n",
            "Human: From which book did you get this quote from?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  This quote is from Albert Einstein\\'s book, \"Ideas and Opinions\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Before executing the following code, make sure to have your\n",
        "# Activeloop key saved in the “ACTIVELOOP_TOKEN” environment variable.\n",
        "\n",
        "# instantiate the LLM and embeddings models\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# create our documents\n",
        "texts = [\n",
        "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
        "    \"Louis XIV was born in 5 September 1638\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# create Deep Lake dataset\n",
        "# TODO: use your organization id here. (by default, org id is your username)\n",
        "my_activeloop_org_id = \"####\"\n",
        "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSv5M4UOjedZ",
        "outputId": "1074b163-c1e3-489c-fc0b-b2d98ad892f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://bakugok/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://bakugok/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (8, 1536)  float32   None   \n",
            "    id        text      (8, 1)      str     None   \n",
            " metadata     json      (8, 1)      str     None   \n",
            "   text       text      (8, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9ee73790-1b6f-11ee-8d95-0242ac1c000c',\n",
              " '9ee7392a-1b6f-11ee-8d95-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm,\n",
        "\tchain_type=\"stuff\",\n",
        "\tretriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "mp-zqcygqnP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "fqD8eJRKq10h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Napoleone born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8xK0zgnq4P0",
        "outputId": "ba975663-a4c5-49bf-8b2d-7a89acca7b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Napoleone was born.\n",
            "Action: Retrieval QA System\n",
            "Action Input: When was Napoleone born?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Napoleon Bonaparte was born on 15 August 1769.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the existing Deep Lake dataset and specify the embedding function\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# create new documents\n",
        "texts = [\n",
        "    \"Lady Gaga was born in 28 March 1986\",\n",
        "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiGDwp6uq53W",
        "outputId": "23090cd3-8ce9-4302-f24f-72e8b0360858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://bakugok/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://bakugok/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            " embedding  embedding  (10, 1536)  float32   None   \n",
            "    id        text      (10, 1)      str     None   \n",
            " metadata     json      (10, 1)      str     None   \n",
            "   text       text      (10, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6ff118f6-1b70-11ee-8d95-0242ac1c000c',\n",
              " '6ff11ae0-1b70-11ee-8d95-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the wrapper class for GPT3\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "# create a retriever from the db\n",
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
        ")\n",
        "\n",
        "# instantiate a tool that uses the retriever\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func=retrieval_qa.run,\n",
        "        description=\"Useful for answering questions.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# create an agent that uses the tool\n",
        "agent = initialize_agent(\n",
        "\ttools,\n",
        "\tllm,\n",
        "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "AGZAE8ShsFGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Michael Jordan born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRiCIp11sIrZ",
        "outputId": "9916943a-b876-4183-9697-409d2e7029ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Michael Jordan was born.\n",
            "Action: Retrieval QA System\n",
            "Action Input: When was Michael Jordan born?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Michael Jordan was born on 17 February 1963.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Michael Jordan was born on 17 February 1963.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Michael Jordan was born on 17 February 1963.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Lady Gaga born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwqE3oA9sL7F",
        "outputId": "e034ea83-7a70-4262-cbf9-8cdcff40e812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Lady Gaga was born.\n",
            "Action: Retrieval QA System\n",
            "Action Input: When was Lady Gaga born?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Lady Gaga was born on 28 March 1986.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Lady Gaga was born on 28 March 1986.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Lady Gaga was born on 28 March 1986.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Google Search API\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "from langchain.agents import AgentType\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper"
      ],
      "metadata": {
        "id": "aWEhkbqhsOoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "search = GoogleSearchAPIWrapper()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"google-search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to search google to answer questions about current events\"\n",
        "    )\n",
        "]\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         verbose=True,\n",
        "                         max_iterations=6)\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         verbose=True,\n",
        "                         max_iterations=6)\n",
        "response = agent(\"What's the latest news about the Mars rover?\")\n",
        "print(response['output'])\n"
      ],
      "metadata": {
        "id": "N-3CRxxvu8zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"Write a summary of the following text: {query}\"\n",
        ")\n",
        "\n",
        "summarize_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "4sfWCKJvvkB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#search = GoogleSearchAPIWrapper()\n",
        "\n",
        "tools = [\n",
        "    # Tool(\n",
        "    #     name=\"Search\",\n",
        "    #     func=search.run,\n",
        "    #     description=\"useful for finding information about recent events\"\n",
        "    # ),\n",
        "    Tool(\n",
        "       name='Summarizer',\n",
        "       func=summarize_chain.run,\n",
        "       description='useful for summarizing texts'\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "zuocFK2Zvzrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "_cTKNpaSv4KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent(\"What's the latest news about the Lady Gaga? Then please summarize the results.\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqqgunSXv9BY",
        "outputId": "6b4b3d29-0af7-4619-d52c-61c05e6fc2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should look for recent news articles about Lady Gaga and then summarize them.\n",
            "Action: Summarizer\n",
            "Action Input: Recent news articles about Lady Gaga\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\n",
            "\n",
            "Lady Gaga has been in the news recently for her upcoming sixth studio album, Chromatica. The album is set to be released on April 10th and will feature collaborations with Ariana Grande, Elton John, and Blackpink. Gaga has also been in the news for her involvement in the fight against the coronavirus pandemic. She has donated $1 million to relief efforts and has been encouraging her fans to stay safe and practice social distancing.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Lady Gaga has been in the news recently for her upcoming sixth studio album, Chromatica, and her involvement in the fight against the coronavirus pandemic. She has donated $1 million to relief efforts and has been encouraging her fans to stay safe and practice social distancing.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Lady Gaga has been in the news recently for her upcoming sixth studio album, Chromatica, and her involvement in the fight against the coronavirus pandemic. She has donated $1 million to relief efforts and has been encouraging her fans to stay safe and practice social distancing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ety0ThfZwTNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8TF8enlwCFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs and Langchain"
      ],
      "metadata": {
        "id": "xIy1ra1WUr4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "# Initialize the LLM\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "\n",
        "# Define the input text\n",
        "input_text = \"\"\"\n",
        "Large Language Models (LLMs) are foundational machine learning models that use deep learning algorithms to process and understand natural language. These models are trained on massive amounts of text data to learn patterns and entity relationships in the language. LLMs can perform many types of language tasks, such as translating languages, analyzing sentiments, chatbot conversations, and more. They can understand complex textual data, identify entities and relationships between them, and generate new text that is coherent and grammatically accurate.\n",
        "What is a Large Language Model?\n",
        "A large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data. These models are capable of generating human-like text and performing various natural language processing tasks.\n",
        "\n",
        "In contrast, the definition of a language model refers to the concept of assigning probabilities to sequences of words, based on the analysis of text corpora. A language model can be of varying complexity, from simple n-gram models to more sophisticated neural network models. However, the term “large language model” usually refers to models that use deep learning techniques and have a large number of parameters, which can range from millions to billions. These models can capture complex patterns in language and produce text that is often indistinguishable from that written by humans.\n",
        "\n",
        "How a Large Language Model Is Built?\n",
        "A large-scale transformer model known as a “large language model” is typically too massive to run on a single computer and is, therefore, provided as a service over an API or web interface. These models are trained on vast amounts of text data from sources such as books, articles, websites, and numerous other forms of written content. By analyzing the statistical relationships between words, phrases, and sentences through this training process, the models can generate coherent and contextually relevant responses to prompts or queries.\n",
        "\n",
        "ChatGPT’s GPT-3 model, for instance, was trained on massive amounts of internet text data, giving it the ability to understand various languages and possess knowledge of diverse topics. As a result, it can produce text in multiple styles. While its capabilities may seem impressive, including translation, text summarization, and question-answering, they are not surprising, given that these functions operate using special “grammars” that match up with prompts.\n",
        "\n",
        "General Architecture\n",
        "The architecture of Large Language Models primarily consists of multiple layers of neural networks, like recurrent layers, feedforward layers, embedding layers, and attention layers. These layers work together to process the input text and generate output predictions.\n",
        "\n",
        "The embedding layer converts each word in the input text into a high-dimensional vector representation. These embeddings capture semantic and syntactic information about the words and help the model to understand the context.\n",
        "The feedforward layers of Large Language Models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. These layers help the model learn higher-level abstractions from the input text.\n",
        "The recurrent layers of LLMs are designed to interpret information from the input text in sequence. These layers maintain a hidden state that is updated at each time step, allowing the model to capture the dependencies between words in a sentence.\n",
        "The attention mechanism is another important part of LLMs, which allows the model to focus selectively on different parts of the input text. This mechanism helps the model attend to the input text’s most relevant parts and generate more accurate predictions.\n",
        "Examples of LLMs\n",
        "Let’s take a look at some popular large language models:\n",
        "\n",
        "GPT-3 (Generative Pre-trained Transformer 3) – This is one of the largest Large Language Models developed by OpenAI. It has 175 billion parameters and can perform many tasks, including text generation, translation, and summarization.\n",
        "BERT (Bidirectional Encoder Representations from Transformers) – Developed by Google, BERT is another popular LLM that has been trained on a massive corpus of text data. It can understand the context of a sentence and generate meaningful responses to questions.\n",
        "XLNet – This LLM developed by Carnegie Mellon University and Google uses a novel approach to language modeling called “permutation language modeling.” It has achieved state-of-the-art performance on language tasks, including language generation and question answering.\n",
        "T5 (Text-to-Text Transfer Transformer) – T5, developed by Google, is trained on a variety of language tasks and can perform text-to-text transformations, like translating text to another language, creating a summary, and question answering.\n",
        "RoBERTa (Robustly Optimized BERT Pretraining Approach) – Developed by Facebook AI Research, RoBERTa is an improved BERT version that performs better on several language tasks.\n",
        "Open Source Large Language Models\n",
        "The availability of open-source LLMs has revolutionized the field of natural language processing, making it easier for researchers, developers, and businesses to build applications that leverage the power of these models to build products at scale for free. One such example is Bloom. It is the first multilingual Large Language Model (LLM) trained in complete transparency by the largest collaboration of AI researchers ever involved in a single research project.\n",
        "\n",
        "With its 176 billion parameters (larger than OpenAI’s GPT-3), BLOOM can generate text in 46 natural languages and 13 programming languages. It is trained on 1.6TB of text data, 320 times the complete works of Shakespeare.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Determine the maximum number of tokens from documentation\n",
        "max_tokens = 4097\n",
        "\n",
        "# Split the input text into chunks based on the max tokens\n",
        "text_chunks = split_text_into_chunks(input_text, max_tokens)\n",
        "\n",
        "# Process each chunk separately\n",
        "results = []\n",
        "for chunk in text_chunks:\n",
        "    result = llm.process(chunk)\n",
        "    results.append(result)\n",
        "\n",
        "# Combine the results as needed\n",
        "final_result = combine_results(results)"
      ],
      "metadata": {
        "id": "K-DCu8bQUub1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPDxlByAVZZR",
        "outputId": "236e57cd-c2a9-4346-8c71-6274776b72b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.10) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Rainbow Socks Co.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    result = llm(\"Tell me a joke\")\n",
        "    print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEv2tpxwVeK3",
        "outputId": "256c5569-1b5d-436c-bb41-78edcc660ad8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Used: 48\n",
            "\tPrompt Tokens: 4\n",
            "\tCompletion Tokens: 44\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.00096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shot Learning"
      ],
      "metadata": {
        "id": "TscSip2fV7EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the weather like?\",\n",
        "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
        "    }, {\n",
        "        \"query\": \"How old are you?\",\n",
        "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "5xuDTFa4VhaL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "\n",
        "# load the model\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"What's the meaning of life?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tDXpPVjnV2vr",
        "outputId": "ad93d9d8-9bde-4e73-84f3-bffd7fe8138b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To find the perfect balance between pizza and ice cream.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QndXKdvAWnvT",
        "outputId": "8cb33381-2b41-43f3-e427-0edf12088720"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "        template=template,\n",
        "    input_variables=['question']\n",
        ")\n",
        "\n",
        "# user question\n",
        "question = \"What is the capital city of Japan?\""
      ],
      "metadata": {
        "id": "5onAKPEZXx_k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize Hub LLM\n",
        "hub_llm = HuggingFaceHub(\n",
        "        repo_id='google/flan-t5-large',\n",
        "    model_kwargs={'temperature':0}\n",
        ")\n",
        "\n",
        "# create prompt template > LLM chain\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=hub_llm\n",
        ")\n",
        "\n",
        "# ask the user question about the capital of France\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixan9OvGX2Im",
        "outputId": "d3d910c6-0d28-4a02-e508-52d03ed85e07"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokyo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = [\n",
        "    {'question': \"What is the capital city of France?\"},\n",
        "    {'question': \"What is the largest mammal on Earth?\"},\n",
        "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
        "    {'question': \"What color is a ripe banana?\"}\n",
        "]\n",
        "res = llm_chain.generate(qa)\n",
        "print( res )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDV8ZHupX52s",
        "outputId": "bf042bf8-0917-4ecc-f2cc-509645f240a0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('02406338-4a54-4e4a-883d-7a4cbe111665'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the capital city of France?\\n\" +\n",
        "    \"What is the largest mammal on Earth?\\n\" +\n",
        "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
        "\t\t\"What color is a ripe banana?\\n\"\n",
        ")\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFnhC1RXeSZB",
        "outputId": "7f80dcb0-6184-4154-c404-a90300494d0d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris\n",
            "Blue Whale\n",
            "Nitrogen\n",
            "Yellow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text summarization"
      ],
      "metadata": {
        "id": "tqLpf2JOewVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "9CnPkDGZecuy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
        "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
        "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
      ],
      "metadata": {
        "id": "JMB_OqEceyKZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
        "summarized_text = summarization_chain.predict(text=text)"
      ],
      "metadata": {
        "id": "_Wsspqeye1Eq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GSswjf5Je22a",
        "outputId": "a989ac2a-478e-49f2-be5b-fc4c2a95c80e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain offers various modules for building language model applications, allowing users to combine them for more complex applications or use them individually for simpler ones, with the basic building block being calling an LLM on input, as demonstrated in the example of creating a company name based on its product.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"A large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data. These models are capable of generating human-like text and performing various natural language processing tasks.\"\n",
        "summarized_text = summarization_chain.predict(text=text)"
      ],
      "metadata": {
        "id": "VdHHZHV6e6QJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "61Nwc02nfEA5",
        "outputId": "a29fc7eb-7b44-4295-98c6-6ec6348c5679"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A large language model is a highly advanced model trained on vast amounts of text data using deep learning techniques, enabling it to generate human-like text and perform various natural language processing tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
        "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
        "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
      ],
      "metadata": {
        "id": "OV-R4a7RfF3s"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_language = \"English\"\n",
        "target_language = \"French\"\n",
        "text = \"What do you know about humans?\"\n",
        "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
      ],
      "metadata": {
        "id": "ho5-qMz3fMjD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tJoMS0QyfVFc",
        "outputId": "574fb186-e8a1-4a88-ef29-7e0de8d111b6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Que savez-vous des humains ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_language = \"English\"\n",
        "target_language = \"Tamil\"\n",
        "text = \"Does earth revolve around sun?\"\n",
        "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
      ],
      "metadata": {
        "id": "6f_G0u-dfXRl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k_yP7PiSfgaZ",
        "outputId": "076693eb-9379-4095-d9cd-80a7b722efc7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'பூமி சூரியனை சுற்றுகிறதா? (Bhūmi sūriyaṉai cuṟṟukiṟatā?)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_language = \"English\"\n",
        "target_language = \"Japanese\"\n",
        "text = \"I am striving to be the greatest!\"\n",
        "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
      ],
      "metadata": {
        "id": "WlZx6MdWfiT_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NTluqaEOf4v5",
        "outputId": "ca56924c-1807-453e-8e34-b3f78e2e0ae5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'私は最高になるために努力しています！ (Watashi wa saikō ni naru tame ni doryoku shiteimasu!)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7hFrsyBf616"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bulding applications powered by LLMs and Langchain"
      ],
      "metadata": {
        "id": "Dounuh8djrOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0)\n",
        "\n",
        "template = \"You are an assitant that helps users find information about animes.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"Find information about the anime {anime_title}.\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "response = chat(chat_prompt.format_prompt(anime_title=\"One Piece\").to_messages())\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuhaBJRFjv9L",
        "outputId": "96fc4332-ac92-44dd-f606-1d9ee975f477"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One Piece is a popular anime series based on the manga of the same name, created by Eiichiro Oda. It first premiered on October 20, 1999, and is still ongoing. The story follows Monkey D. Luffy, a young pirate with the ability to stretch his body like rubber after eating a Devil Fruit. Luffy sets out on a journey to find the legendary treasure known as One Piece and become the Pirate King.\n",
            "\n",
            "The anime is known for its vast world-building, unique characters, and epic adventures. It takes place in a world where pirates roam the seas, and Luffy, along with his crew, the Straw Hat Pirates, encounters various challenges, battles powerful enemies, and explores different islands and territories.\n",
            "\n",
            "One Piece has gained immense popularity worldwide and has become one of the longest-running and highest-grossing anime series of all time. It has spawned numerous movies, spin-offs, video games, and merchandise. The anime has a mix of action, comedy, drama, and emotional moments, making it appealing to a wide range of audiences.\n",
            "\n",
            "If you are interested in watching One Piece, you can find it on various streaming platforms like Crunchyroll, Funimation, and Netflix, depending on your region. The series currently has over 900 episodes, so be prepared for a long and exciting journey through the Grand Line!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZjQvsXGlVF0",
        "outputId": "6cb92d49-dd2f-4bcb-fc9e-362d52336b9e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.12.1-py3-none-any.whl (254 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/254.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.8/254.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Initialize language model\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "# Load the summarization chain\n",
        "summarize_chain = load_summarize_chain(llm)\n",
        "\n",
        "# Load the document using PyPDFLoader\n",
        "document_loader = PyPDFLoader(file_path=\"path/to/your/pdf/file.pdf\")\n",
        "document = document_loader.load()\n",
        "\n",
        "# Summarize the document\n",
        "summary = summarize_chain(document)\n",
        "print(summary['output_text'])"
      ],
      "metadata": {
        "id": "zF4xOXCCmjLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}